---
layout: post
title:  "概率论-随机事件与概率"
categories: 机器学习
tags: 机器学习 数学基础 概率论
author: naiixeel
---

* content
{:toc}

### 乘法规则

有一个复合试验由多个步骤组成，如果第一个步骤有 $n_1$ 个可能的结果，第一个步骤有 $n_2$ 个可能的结果，……，第 $r$ 个步骤有 $n_r$ 个可能的结果，那么整个实验有 $n = \prod_{i=1}^r n_r$ 个可能的结果。

### 抽样结果

在一个大小为 $n​$ 的总体中取一个大小为 $k​$ 的样本，在不同的抽样条件下可能的结果数量如下表所示：

|       | 有序 | 无序 |
| :--:  | :--: | :--: |
| 有放回 | $n^k$ | $C_{n+k-1}^k=\frac{(n+k-1)!}{k!(n-1)!}$ |
| 无放回 | $A_n^k=\frac{n!}{(n-k)!}$ | $C_n^k=\frac{n!}{k!(n-k)!}$ |

### 概率的简单定义

如果试验中所有可能出现的基本事件只有有限个并且每个基本事件的出现都是等可能的，那么事件 A 发生的概率是
$$
P(A)=\frac{A包含的基本事件的个数}{基本事件的总数}
$$


### 独立性

设 $A,B​$ 为随机事件，如果不论事件 $A​$ 是否发生都不影响事件 $B​$ 的发生概率，那么事件 $A​$ 和 $B​$ 相互独立。当且仅当以下等式成立时，事件 $A​$ 和 $B​$ 相互独立：

$$
\begin{align}
P(AB)&=P(A)P(B)\\
P(A\mid B)&=P(A)\\
P(B\mid A)&=P(B)
\end{align}
$$

给定第三个事件 $C$，如果有 $P(AB \mid C)=P(A\mid C)P(B\mid C)$，则称事件 $A$ 和 $B$ 条件独立。事件 $A$ 和 $B$ 条件独立并不意味着它们相互独立，事件 $A$ 和 $B$ 相互独立也并不意味着它们条件独立。

### 联合概率

联合概率是指在多元的概率分布中多个随机变量分别满足各自条件的概率，也就是多个事件同时发生的概率。随机事件 $A,B$ 的联合概率可记为 $P(AB)$，P(A,B) 或 $P(A\cap B)$。

### 边缘概率

边缘概率是某个事件单独发生的概率，与其它事件无关。在联合概率中，把最终结果中不需要的那些事件合并成其事件的全概率就得到了某个事件的边缘概率。随机事件 $A$ 的边缘概率记为 $P(A)$。

### 条件概率

条件概率是指事件 $A$ 在另外一个事件 $B$ 已经发生的条件下发生概率，记为 $P(A \mid B)$。条件概率也是一种概率，满足所有的概率理论和运算规则。条件概率可由联合概率和边缘概率得到 $P(A \mid B)=P(AB)/P(B)$。

### 辛普森悖论

辛普森悖论是指在分组比较中都占优势的一方，在总评中有时反而是失势的一方。即对于事件 $A,B,C$ 可能存在 $P(A \mid BC) < P(A \mid B\overline{C})$ 且 $P(A \mid B\overline{C})<P(A \mid \overline{B}\overline{C})$ 但是 $P(A\mid B)> P(A\mid \overline{B})$ 的情况。

### 全概率公式

如果事件 $B_1,B_2,\cdots,B_n$ 构成一个完备事件组，即它们两两互不，其和为全集；并且 $P(B_i)>0$，则对于任一事件 $A$ 有

$$
P(A)=P(A\mid B_1)P(B_1)+P(A\mid B_2)P(B_2)+\cdots+P(A\mid B_n)P(B_n)
$$

对于任意两随机事件 $A$ 和 $B$，如果 $B$ 和 $\overline{B}$ 构成完备事件组，则 $P(A)=P(A\mid B)P(B)+P(A\mid\overline{B})P(\overline{B})​$ 成立。

### 贝叶斯公式

贝叶斯公式是建立在条件概率的基础上寻找事件发生的原因（即大事件 $A​$ 已经发生的条件下，分割中的小事件 $B_i​$ 的概率)，设 $B_1,B_2,\cdots,B_n​$ 是样本空间 $\Omega​$ 的一个划分，则对任一事件 $A​$ ($P(A)>0​$)，有

$$
P(B_i\mid A)=\frac{P(B_i)P(A\mid B_i)}{\sum_{j=1}^nP(B_j)P(A\mid B_j)}=\frac{P(B_i)P(A\mid B_i)}{P(A)}
$$

其中，$B_i$ 是导致试验结果 $A$ 发生的原因，$P(B_i)$ 表示 $B_i$ 发生的可能性大小，称为 $B_i$ 的先验概率(Prior)。$P(B_i \mid A)$ 称为 $B_i$ 的后验概率(Posterior)，即在试验结果 $A$ 发生后，对 $B_i$ 事件概率的重新评估。$P(A \mid B_i)/P(A)$ 称为可能性函数(Likelyhood)，这是一个调整因子，使得预估概率更接近真实概率。贝叶斯公式可以理解为

$$
后验概率　＝　先验概率 \times 调整因子
$$

表示先预估一个先验概率，然后加入实验结果，看这个实验到底是增强还是削弱了先验概率，由此得到更接近事实的后验概率。如果可能性函数 $P(A \mid B_i)/P(A)>1​$，意味着先验概率被增强，事件 $B_i​$ 的发生的可能性变大；如果可能性函数 $P(A \mid B_i)/P(A)=1​$，意味着试验结果 $A​$ 无助于判断事件$B_i​$ 的可能性；如果可能性函数 $P(A \mid B_i)/P(A)<1​$，意味着先验概率被削弱，事件 $B_i​$ 的可能性变小。在运用贝叶斯公式时，一般已知和未知条件为：

> (1) 哪种原因产生试验结果 $A​$ 是未知的，但是每种原因发生的概率已知，即 $P(B_i)​$ ；
>
> (2) 试验结果 $A$ 是已经发生的确定事实，且每种原因导致 $A$ 发生的概率已知，即$P(A \mid B_i)​$；
>
> (3) 试验结果 $A$ 发生的概率 $P(A)​$ 未知，需要使用全概率公式计算得到；
>
> (4) 求解的目标是用事件 $B_i$ 的无条件概率求其在 $A$ 发生的条件下的有条件概率 $P(B_i\mid A)$。

全概率公式可看做是由原因推知结果，而贝叶斯公式则恰好相反，其作用在于由结果推原因，即：

$$
P(B|A)=\frac{P(B)P(A\mid B)}{P(A)} \quad \Rightarrow \quad P(因\mid 果)=\frac{P(因)P(果\mid 因)}{P(果)}
$$

### 参考文献

[ [1](https://static1.squarespace.com/static/54bf3241e4b0f0d81bf7ff36/t/55e9494fe4b011aed10e48e5/1441352015658/probability_cheatsheet.pdf) ] probability cheatsheet

[ [2](https://www.zhoulujun.cn/html/theory/math/2017_0913_8050.html) ] 贝叶斯公式由浅入深大讲解—AI基础算法入门

[ [3](https://www.cnblogs.com/Belter/p/5923828.html) ] 【概率论与数理统计】全概率公式和贝叶斯公式